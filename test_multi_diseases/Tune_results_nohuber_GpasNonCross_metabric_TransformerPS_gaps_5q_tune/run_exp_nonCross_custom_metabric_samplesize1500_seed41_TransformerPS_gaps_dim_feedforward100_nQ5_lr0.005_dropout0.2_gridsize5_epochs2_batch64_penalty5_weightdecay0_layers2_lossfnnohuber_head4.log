python: can't open file '/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/run_exp.ekan.nonCrossgaps_gpu.py': [Errno 2] No such file or directory
Parsed Arguments:
dataset: custom
nhead: 4
dropout: 0.2
n_quantiles: 5
batch_size: 64
n_epoch: 2
grid_size: 5
lr: 0.005
weight_decay: 0.0
device: gpu
seed: 41
verbose: 1
loss_fn: nohuber
expname: nohuber_TransformerPS_gaps_metabric_tune
d: 100
dataset_str: metabric
model_type: TransformerPS_gaps
quantiles: [0.1, 0.25, 0.5, 0.75, 0.9]
n: 1500
theta: 180.0
layers: 2
jpg_name: run_exp_nonCross_custom_metabric_samplesize1500_seed41_TransformerPS_gaps_dim_feedforward100_nQ5_lr0.005_dropout0.2_gridsize5_epochs2_batch64_penalty5_weightdecay0_layers2_lossfnnohuber_head4
result_path: /work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/test_multi_diseases/Tune_results_nohuber_GpasNonCross_metabric_TransformerPS_gaps_5q_tune
acfn: relu
penalty: 5
torch.cuda.is_available(): False
Using device: cpu
num_diseases: 1
Transformer output dim: 5
TransformerLayer 100 4 2 0.2 relu
DeepQuantReg(
  (layer_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
  (sigmoid): Sigmoid()
  (relu): ReLU()
  (model_base): TransformerPS(
    (input_embed): Linear(in_features=1, out_features=100, bias=True)
    (position_encodeT): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
          )
          (linear1): Linear(in_features=100, out_features=200, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=200, out_features=100, bias=True)
          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (output_linear): Linear(in_features=900, out_features=100, bias=True)
  )
  (output_layer): Linear(in_features=100, out_features=5, bias=True)
)
Epoch 0, Loss: 0.6258234947919845
Traceback (most recent call last):
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/main.py", line 196, in <module>
    main()
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/main.py", line 155, in main
    result = deep_quantreg(
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/exp/train.py", line 266, in deep_quantreg
    train_loss = loss_fn(np.log(Y_train), torch.tensor(train_predictions), W_train)
  File "/nas/longleaf/home/shuaishu/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/nas/longleaf/home/shuaishu/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/utils/helpers.py", line 104, in forward
    batch_size, num_diseases, num_quantiles = y_pred.shape
ValueError: not enough values to unpack (expected 3, got 2)
Parsed Arguments:
dataset: custom
nhead: 4
dropout: 0.2
n_quantiles: 5
batch_size: 64
n_epoch: 2
grid_size: 5
lr: 0.005
weight_decay: 0.0
device: gpu
seed: 41
verbose: 1
loss_fn: nohuber
expname: nohuber_TransformerPS_gaps_metabric_tune
d: 100
dataset_str: metabric
model_type: TransformerPS_gaps
quantiles: [0.1, 0.25, 0.5, 0.75, 0.9]
n: 1500
theta: 180.0
layers: 2
jpg_name: run_exp_nonCross_custom_metabric_samplesize1500_seed41_TransformerPS_gaps_dim_feedforward100_nQ5_lr0.005_dropout0.2_gridsize5_epochs2_batch64_penalty5_weightdecay0_layers2_lossfnnohuber_head4
result_path: /work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/test_multi_diseases/Tune_results_nohuber_GpasNonCross_metabric_TransformerPS_gaps_5q_tune
acfn: relu
penalty: 5
torch.cuda.is_available(): False
Using device: cpu
num_diseases: 1
Transformer output dim: 5
TransformerLayer 100 4 2 0.2 relu
DeepQuantReg(
  (layer_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
  (sigmoid): Sigmoid()
  (relu): ReLU()
  (model_base): TransformerPS(
    (input_embed): Linear(in_features=1, out_features=100, bias=True)
    (position_encodeT): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
          )
          (linear1): Linear(in_features=100, out_features=200, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=200, out_features=100, bias=True)
          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (output_linear): Linear(in_features=900, out_features=100, bias=True)
  )
  (output_layer): Linear(in_features=100, out_features=5, bias=True)
)
Epoch 0, Loss: 0.5340504117310048
Traceback (most recent call last):
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/main.py", line 196, in <module>
    main()
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/main.py", line 155, in main
    result = deep_quantreg(
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/exp/train.py", line 266, in deep_quantreg
    train_loss = loss_fn(np.log(Y_train), torch.tensor(train_predictions), W_train)
  File "/nas/longleaf/home/shuaishu/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/nas/longleaf/home/shuaishu/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/utils/helpers.py", line 104, in forward
    batch_size, num_diseases, num_quantiles = y_pred.shape
ValueError: not enough values to unpack (expected 3, got 2)
Parsed Arguments:
dataset: custom
nhead: 4
dropout: 0.2
n_quantiles: 5
batch_size: 64
n_epoch: 2
grid_size: 5
lr: 0.005
weight_decay: 0.0
device: gpu
seed: 41
verbose: 1
loss_fn: nohuber
expname: nohuber_TransformerPS_gaps_metabric_tune
d: 100
dataset_str: metabric
model_type: TransformerPS_gaps
quantiles: [0.1, 0.25, 0.5, 0.75, 0.9]
n: 1500
theta: 180.0
layers: 2
jpg_name: run_exp_nonCross_custom_metabric_samplesize1500_seed41_TransformerPS_gaps_dim_feedforward100_nQ5_lr0.005_dropout0.2_gridsize5_epochs2_batch64_penalty5_weightdecay0_layers2_lossfnnohuber_head4
result_path: /work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/test_multi_diseases/Tune_results_nohuber_GpasNonCross_metabric_TransformerPS_gaps_5q_tune
acfn: relu
penalty: 5
torch.cuda.is_available(): False
Using device: cpu
num_diseases: 1
Transformer output dim: 5
TransformerLayer 100 4 2 0.2 relu
DeepQuantReg(
  (layer_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
  (sigmoid): Sigmoid()
  (relu): ReLU()
  (model_base): TransformerPS(
    (input_embed): Linear(in_features=1, out_features=100, bias=True)
    (position_encodeT): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
          )
          (linear1): Linear(in_features=100, out_features=200, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=200, out_features=100, bias=True)
          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (output_linear): Linear(in_features=900, out_features=100, bias=True)
  )
  (output_layer): Linear(in_features=100, out_features=5, bias=True)
)
Epoch 0, Loss: 0.720756185054779
Traceback (most recent call last):
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/main.py", line 196, in <module>
    main()
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/main.py", line 155, in main
    result = deep_quantreg(
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/exp/train.py", line 266, in deep_quantreg
    train_loss = loss_fn(np.log(Y_train), torch.tensor(train_predictions), W_train)
  File "/nas/longleaf/home/shuaishu/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/nas/longleaf/home/shuaishu/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/utils/helpers.py", line 104, in forward
    batch_size, num_diseases, num_quantiles = y_pred.shape
ValueError: not enough values to unpack (expected 3, got 2)
Parsed Arguments:
dataset: custom
nhead: 4
dropout: 0.2
n_quantiles: 5
batch_size: 64
n_epoch: 2
grid_size: 5
lr: 0.005
weight_decay: 0.0
device: gpu
seed: 41
verbose: 1
loss_fn: nohuber
expname: nohuber_TransformerPS_gaps_metabric_tune
d: 100
dataset_str: metabric
model_type: TransformerPS_gaps
quantiles: [0.1, 0.25, 0.5, 0.75, 0.9]
n: 1500
theta: 180.0
layers: 2
jpg_name: run_exp_nonCross_custom_metabric_samplesize1500_seed41_TransformerPS_gaps_dim_feedforward100_nQ5_lr0.005_dropout0.2_gridsize5_epochs2_batch64_penalty5_weightdecay0_layers2_lossfnnohuber_head4
result_path: /work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/test_multi_diseases/Tune_results_nohuber_GpasNonCross_metabric_TransformerPS_gaps_5q_tune
acfn: relu
penalty: 5
Y:  (1217, 1)
Y:  (305, 1)
Y:  (381, 1)
torch.cuda.is_available(): False
Using device: cpu
num_diseases: 1
Transformer output dim: 5
TransformerLayer 100 4 2 0.2 relu
DeepQuantReg(
  (layer_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
  (sigmoid): Sigmoid()
  (relu): ReLU()
  (model_base): TransformerPS(
    (input_embed): Linear(in_features=1, out_features=100, bias=True)
    (position_encodeT): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
          )
          (linear1): Linear(in_features=100, out_features=200, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=200, out_features=100, bias=True)
          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (output_linear): Linear(in_features=900, out_features=100, bias=True)
  )
  (output_layer): Linear(in_features=100, out_features=5, bias=True)
)
Epoch 0, Loss: 0.6669024735689163
Traceback (most recent call last):
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/main.py", line 196, in <module>
    main()
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/main.py", line 155, in main
    result = deep_quantreg(
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/exp/train.py", line 266, in deep_quantreg
    train_loss = loss_fn(np.log(Y_train), torch.tensor(train_predictions), W_train)
  File "/nas/longleaf/home/shuaishu/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/nas/longleaf/home/shuaishu/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/utils/helpers.py", line 104, in forward
    batch_size, num_diseases, num_quantiles = y_pred.shape
ValueError: not enough values to unpack (expected 3, got 2)
Parsed Arguments:
dataset: custom
nhead: 4
dropout: 0.2
n_quantiles: 5
batch_size: 64
n_epoch: 2
grid_size: 5
lr: 0.005
weight_decay: 0.0
device: gpu
seed: 41
verbose: 1
loss_fn: nohuber
expname: nohuber_TransformerPS_gaps_metabric_tune
d: 100
dataset_str: metabric
model_type: TransformerPS_gaps
quantiles: [0.1, 0.25, 0.5, 0.75, 0.9]
n: 1500
theta: 180.0
layers: 2
jpg_name: run_exp_nonCross_custom_metabric_samplesize1500_seed41_TransformerPS_gaps_dim_feedforward100_nQ5_lr0.005_dropout0.2_gridsize5_epochs2_batch64_penalty5_weightdecay0_layers2_lossfnnohuber_head4
result_path: /work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/test_multi_diseases/Tune_results_nohuber_GpasNonCross_metabric_TransformerPS_gaps_5q_tune
acfn: relu
penalty: 5
Y:  (1217, 1)
Y:  (305, 1)
Y:  (381, 1)
torch.cuda.is_available(): False
Using device: cpu
num_diseases: 1
Transformer output dim: 5
TransformerLayer 100 4 2 0.2 relu
DeepQuantReg(
  (layer_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
  (sigmoid): Sigmoid()
  (relu): ReLU()
  (model_base): TransformerPS(
    (input_embed): Linear(in_features=1, out_features=100, bias=True)
    (position_encodeT): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
          )
          (linear1): Linear(in_features=100, out_features=200, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=200, out_features=100, bias=True)
          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (output_linear): Linear(in_features=900, out_features=100, bias=True)
  )
  (output_layer): Linear(in_features=100, out_features=5, bias=True)
)
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([1, 1, 5])
Epoch 0, Loss: 0.6422977954149246
Traceback (most recent call last):
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/main.py", line 196, in <module>
    main()
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/main.py", line 155, in main
    result = deep_quantreg(
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/exp/train.py", line 266, in deep_quantreg
    train_loss = loss_fn(np.log(Y_train), torch.tensor(train_predictions), W_train)
  File "/nas/longleaf/home/shuaishu/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1739, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/nas/longleaf/home/shuaishu/.local/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1750, in _call_impl
    return forward_call(*args, **kwargs)
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/utils/helpers.py", line 104, in forward
    batch_size, num_diseases, num_quantiles = y_pred.shape
ValueError: not enough values to unpack (expected 3, got 2)
Parsed Arguments:
dataset: custom
nhead: 4
dropout: 0.2
n_quantiles: 5
batch_size: 64
n_epoch: 2
grid_size: 5
lr: 0.005
weight_decay: 0.0
device: gpu
seed: 41
verbose: 1
loss_fn: nohuber
expname: nohuber_TransformerPS_gaps_metabric_tune
d: 100
dataset_str: metabric
model_type: TransformerPS_gaps
quantiles: [0.1, 0.25, 0.5, 0.75, 0.9]
n: 1500
theta: 180.0
layers: 2
jpg_name: run_exp_nonCross_custom_metabric_samplesize1500_seed41_TransformerPS_gaps_dim_feedforward100_nQ5_lr0.005_dropout0.2_gridsize5_epochs2_batch64_penalty5_weightdecay0_layers2_lossfnnohuber_head4
result_path: /work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/test_multi_diseases/Tune_results_nohuber_GpasNonCross_metabric_TransformerPS_gaps_5q_tune
acfn: relu
penalty: 5
Y:  (1217, 1)
Y:  (305, 1)
Y:  (381, 1)
torch.cuda.is_available(): False
Using device: cpu
num_diseases: 1
Transformer output dim: 5
TransformerLayer 100 4 2 0.2 relu
DeepQuantReg(
  (layer_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
  (sigmoid): Sigmoid()
  (relu): ReLU()
  (model_base): TransformerPS(
    (input_embed): Linear(in_features=1, out_features=100, bias=True)
    (position_encodeT): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
          )
          (linear1): Linear(in_features=100, out_features=200, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=200, out_features=100, bias=True)
          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (output_linear): Linear(in_features=900, out_features=100, bias=True)
  )
  (output_layer): Linear(in_features=100, out_features=5, bias=True)
)
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([1, 1, 5])
Epoch 0, Loss: 0.5593429945409298
y_pred: torch.Size([1217, 1, 5])
y_pred: torch.Size([305, 1, 5])
Epoch 0, Train; Loss: 0.25962358713150024, True MSE: 0, True MSE 50 quantile: 0, MMSE: 2.381011962890625, CI: 0.5027849049276729
Epoch 0, Valid; Loss: 0.2654820680618286, True MSE: 0, True MSE 50 quantile: 0, MMSE: 2.4928030967712402, CI: 0.5676259662848094
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([1, 1, 5])
Epoch 1, Loss: 0.3275498501956463
y_pred: torch.Size([1217, 1, 5])
y_pred: torch.Size([305, 1, 5])
Epoch 1, Train; Loss: 0.32304641604423523, True MSE: 0, True MSE 50 quantile: 0, MMSE: 1.4407883882522583, CI: 0.5079287368665065
Epoch 1, Valid; Loss: 0.3314354717731476, True MSE: 0, True MSE 50 quantile: 0, MMSE: 1.5246611833572388, CI: 0.5244854242339574
Traceback (most recent call last):
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/main.py", line 196, in <module>
    main()
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/main.py", line 155, in main
    result = deep_quantreg(
  File "/work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/exp/train.py", line 381, in deep_quantreg
    train_predictions = convert_d(model(X_train).squeeze())
NameError: name 'convert_d' is not defined
Parsed Arguments:
dataset: custom
nhead: 4
dropout: 0.2
n_quantiles: 5
batch_size: 64
n_epoch: 2
grid_size: 5
lr: 0.005
weight_decay: 0.0
device: gpu
seed: 41
verbose: 1
loss_fn: nohuber
expname: nohuber_TransformerPS_gaps_metabric_tune
d: 100
dataset_str: metabric
model_type: TransformerPS_gaps
quantiles: [0.1, 0.25, 0.5, 0.75, 0.9]
n: 1500
theta: 180.0
layers: 2
jpg_name: run_exp_nonCross_custom_metabric_samplesize1500_seed41_TransformerPS_gaps_dim_feedforward100_nQ5_lr0.005_dropout0.2_gridsize5_epochs2_batch64_penalty5_weightdecay0_layers2_lossfnnohuber_head4
result_path: /work/users/s/h/shuaishu/Intern/Servier/DeepQuantreg/To_Pytorch/DeepQuantreg/project/test_multi_diseases/Tune_results_nohuber_GpasNonCross_metabric_TransformerPS_gaps_5q_tune
acfn: relu
penalty: 5
Y:  (1217, 1)
Y:  (305, 1)
Y:  (381, 1)
torch.cuda.is_available(): False
Using device: cpu
num_diseases: 1
Transformer output dim: 5
TransformerLayer 100 4 2 0.2 relu
DeepQuantReg(
  (layer_norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
  (sigmoid): Sigmoid()
  (relu): ReLU()
  (model_base): TransformerPS(
    (input_embed): Linear(in_features=1, out_features=100, bias=True)
    (position_encodeT): PositionalEncoding(
      (dropout): Dropout(p=0.2, inplace=False)
    )
    (norm): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0-1): 2 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=100, out_features=100, bias=True)
          )
          (linear1): Linear(in_features=100, out_features=200, bias=True)
          (dropout): Dropout(p=0.2, inplace=False)
          (linear2): Linear(in_features=200, out_features=100, bias=True)
          (norm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.2, inplace=False)
          (dropout2): Dropout(p=0.2, inplace=False)
        )
      )
    )
    (output_linear): Linear(in_features=900, out_features=100, bias=True)
  )
  (output_layer): Linear(in_features=100, out_features=5, bias=True)
)
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([1, 1, 5])
Epoch 0, Loss: 0.5360489502549172
y_pred: torch.Size([1217, 1, 5])
y_pred: torch.Size([305, 1, 5])
Epoch 0, Train; Loss: 0.4140453338623047, True MSE: 0, True MSE 50 quantile: 0, MMSE: 3.9458630084991455, CI: 0.47917780318235226
Epoch 0, Valid; Loss: 0.4233957827091217, True MSE: 0, True MSE 50 quantile: 0, MMSE: 4.1410322189331055, CI: 0.48746080531495456
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([64, 1, 5])
y_pred: torch.Size([1, 1, 5])
Epoch 1, Loss: 0.30677360445261004
y_pred: torch.Size([1217, 1, 5])
y_pred: torch.Size([305, 1, 5])
Epoch 1, Train; Loss: 0.25309568643569946, True MSE: 0, True MSE 50 quantile: 0, MMSE: 1.7373840808868408, CI: 0.5017142143877509
Epoch 1, Valid; Loss: 0.2603614628314972, True MSE: 0, True MSE 50 quantile: 0, MMSE: 1.8478209972381592, CI: 0.5186271770513179
/nas/longleaf/home/shuaishu/.local/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.
  return _methods._mean(a, axis=axis, dtype=dtype,
Observed CI censored for all quantile: test nan
Observed CI IPCW for all quantile: test nan
Observed CI censored for all quantile: valid nan
Observed CI IPCW for all quantile: valid nan
Observed average MMSE for all quantile: train 2.0684147 valid 2.202704 test 2.3264673
Observed average CI for all quantile: train 0.5762086380396299 valid 0.5559746670392102 test 0.5700416778189883
Observed average QL for all quantile: train 0.25418797 valid 0.25525594 test 0.24644855
Observed MMSE for each quantile: train Q0.1: 1.1401 | Q0.25: 0.8093 | Q0.5: 1.2679 | Q0.75: 2.3503 | Q0.9: 4.7745
Observed CI for each quantile: train Q0.1: 0.5597 | Q0.25: 0.5821 | Q0.5: 0.5803 | Q0.75: 0.5920 | Q0.9: 0.5670
Observed QL for each quantile: train Q0.1: 0.1881 | Q0.25: 0.3439 | Q0.5: 0.3579 | Q0.75: 0.2295 | Q0.9: 0.1515
Observed MMSE for each quantile: test Q0.1: 1.1962 | Q0.25: 1.0071 | Q0.5: 1.5271 | Q0.75: 2.6860 | Q0.9: 5.2159
Observed CI for each quantile: test Q0.1: 0.5612 | Q0.25: 0.5773 | Q0.5: 0.5746 | Q0.75: 0.5797 | Q0.9: 0.5574
Observed QL for each quantile: test Q0.1: 0.1736 | Q0.25: 0.3400 | Q0.5: 0.3347 | Q0.75: 0.2291 | Q0.9: 0.1549
Observed MMSE for each quantile: valid Q0.1: 1.1684 | Q0.25: 0.9119 | Q0.5: 1.4028 | Q0.75: 2.5254 | Q0.9: 5.0050
Observed CI for each quantile: valid Q0.1: 0.5429 | Q0.25: 0.5661 | Q0.5: 0.5629 | Q0.75: 0.5677 | Q0.9: 0.5404
Observed QL for each quantile: valid Q0.1: 0.1993 | Q0.25: 0.3441 | Q0.5: 0.3564 | Q0.75: 0.2215 | Q0.9: 0.1550
